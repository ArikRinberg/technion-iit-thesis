\section{Preliminaries}
\label{ivl-sec:preliminaries}

Section~\ref{ivl-ssec:det-objects} discusses deterministic shared memory objects and defines linearizability.
% the de facto standard correctness criterion,
%In Section~\ref{ivl-ssec:uni-step-complex} we define uniform step complexity, and in
In Section~\ref{ivl-ssec:rand-objects} we discuss randomized algorithms and their correctness criteria.

\subsection{Deterministic objects}
\label{ivl-ssec:det-objects}

We consider a standard shared memory model~\cite{herlihy1990linearizability}, where a set of \emph{asynchronous}
\emph{processes} access atomic shared memory variables. Accessing these
shared variables is instantaneous.
Processes take \emph{steps} according to an \emph{algorithm}, which is a deterministic
state machine, where a step can access a shared memory variable, do local computations, and possibly return
some value. \inred{A step may also alter the \emph{state}, which is it the collection of local and shared variables.}
 An \emph{execution} of an algorithm is an alternating
sequence of steps and states. We focus on algorithms that
implement \emph{objects}, which support
\emph{operations}, such as {\sc read} and {\sc write}. Operations begin
with an \emph{invocation} step and end with a \emph{response} step.
A \emph{schedule}, denoted $\sigma$, is the order in
which processes take steps, and the operations they invoke
in invoke steps with their parameters.
Because we consider deterministic algorithms,
$\sigma$ uniquely defines an execution of a given algorithm.

A \emph{history} is the sequence of invoke and response steps in an execution. Given an algorithm $A$ and
a schedule $\sigma$, $H(A, \sigma)$ is the history of the execution of $A$ with
schedule $\sigma$. A \emph{sequential} history is an alternating sequence of
invocations and their responses, beginning with an invoke step.
We denote the return value of operation $op$ with parameter $arg$ in history $H$ by $\text{ret}(x.op,H)$,
\inred{where $x$ is the object exposing operation $op$}.
We refer to the invocation step of operation $op$ with parameter $arg$
by process $p$ as $inv_p(x.op(arg))$ and to its response
step by $rsp_p(x.op(arg)\rightarrow ret)$, where $ret = \text{ret}(x.op,H)$, \inred{and where
$x$ in the object exposing operation $op$}. \inred{We omit $x$ and $arg$ when obvious from context.} A history
defines a partial order on operations: Operation $op_1$ \emph{precedes} $op_2$
in history $H$, denoted $op_1 \prec_H op_2$, if $rsp(op_1)$ precedes $inv(op_2(arg))$
in $H$. Two operations are \emph{concurrent} if neither precedes the other.

A \emph{well-formed} history is one that does not contain concurrent operations by the same process, and
where every response event for operation $op$ is preceded by an invocation of the same operation.
A schedule is well-formed if it gives rise to a well-formed history, and an execution
is well-formed if it is based on a well-formed schedule.
% The projection of a history $H$ onto an object $x$, denoted $H|_x$, is the
We denote by $H|_x$ the sub-history of $H$ consisting only of invocations and responses
on object $x$. Operation $op$ is \emph{pending} in a history $H$ if $op$ is invoked
in $H$ but does not return.

% A \emph{schedule}, generally denoted $\sigma$, is the order in
% which processes invoke operations or take steps. Given a deterministic algorithm, the schedule defines
% a history. A \emph{well-formed} schedule does not invoke concurrent operations by the same
% process, i.e., defines a well formed history.
% For some $op_i$, we define $\text{ret}(H, op_i)$ to be the return value of $op_i$
% in history $H$. The schedule is generated by the \emph{adversary}. For simplicity,
% we assume an \emph{oblivious} adversary, this adversary generates the schedule
% regardless of the observed coin flip vector.
% An object supports \emph{operations}, which are
% delimited by two events \emph{invoke} and \emph{response}.


% A \emph{register} supports the {\sc read} and {\sc write}($v$) operations.
% The response may return a value. A \emph{single writer multi reader (SWMR)} register has a predefined
% process which is the only one that is allowed to invoke a {\sc write} operation, but all processes may
% invoke a {\sc read} operation.


% %An invoke of operation $op$ by process $p$ is denoted $i_p^{op}$, similarly a response is denoted $r_p^{op}$.
% A \emph{history} $H$ is a sequence of invoke and response events. An example of a history with processes
% $p,q$ and a SWMR register which only process $p$ may write to can be $H=\mathit{inv}_p\text{\sc write}(3),
% \mathit{inv}_q\text{\sc read}, \mathit{rsp}_q\text{\sc read} \rightarrow 3, \mathit{rsp}_p\text{\sc write}$.
% For two operations $op_1,op_2$ in $H$, we say that $op_1$ \emph{precedes} $op_2$ in $H$, denoted
% $op_1 \prec_H op_2$, if the return even of $op_1$ precedes the invoke event of $op_2$ in $H$.
% Two operations are \emph{concurrent} if neither precedes the other.
% A \emph{well formed} history does not contain concurrent operations by the same process, and
% every response event for operation $op$ is preceded by an invocation of the same operation.
% The projection of a history $H$ onto and object $x$, denoted $H|_x$, is the sub-history consisting only of invocations and responses
% of operations on $x$.
% A \emph{sequential history} is a history in which each invocation is immediately followed
% by its response. The
% \emph{sequential specification} of an object is the set of its allowed sequential histories,
% denoted $\mathcal H$.

% Processes invoking operations take \emph{steps} according to an \emph{algorithm}, which is a deterministic
% state machine, where a step can access a shared variable, do local computations, and possibly return.
% A \emph{schedule}, generally denoted $\sigma$, is the order in
% which processes invoke operations or take steps. Given a deterministic algorithm, the schedule defines
% a history. A \emph{well formed} schedule does not invoke concurrent operations by the same
% process, i.e., defines a well formed history.
% For some $op_i$, we define $\text{ret}(H, op_i)$ to be the return value of $op_i$
% in history $H$. The schedule is generated by the \emph{adversary}. For simplicity,
% we assume an \emph{oblivious} adversary, this adversary generates the schedule
% regardless of the observed coin flip vector.

% Given a history $H$, an operation $op$ is said to be \emph{complete} if $H$ contains both
% $\mathit{inv}(op)$ and the corresponding $\mathit{rsp}(op)$. A history $H$ is said to
% be complete if all operations in it are complete, otherwise it is said to be incomplete.
% A well formed incomplete history $H$ can be completed to a history $H'$ by either removing
% incomplete operations or adding an appropriate response event, such that $\prec_H'$ 
% extends $\prec_H$. Given a history $H$, we denote the set of completed histories
% as $\mathit{complete}(H)$.


Correctness of an object's implementation is defined with respect to a
sequential specification $\mathcal{H}$, which is the object's set of allowed sequential histories.
If the history spans multiple objects, $\mathcal{H}$ consists of sequential histories $H$ such that for all
objects $x$, $H|_x$ pertains to $x$'s sequential specification (denoted $\mathcal{H}_x$).
A \emph{linearization}~\cite{herlihy1990linearizability} of a concurrent history $H$ is a
sequential history $H'$ such that (1) after removing some pending
operations from $H$ and completing others, it contains the same invocations and
responses as $H'$ with the same parameters and return values, and (2) $H'$
preserves the partial order $\prec_H$.
Note that our definition of linearization diverges from the one in~\cite{herlihy1990linearizability} in that
it is not associated with any sequential specification; instead we require that
the linearization pertain to the sequential specification when defining linearizability as follows:
Algorithm $A$ is a \emph{linearizable implementation}
of a sequential specification $\mathcal{H}$ if every history of a
well-formed execution of $A$ has a linearization in $\mathcal{H}$.

% If every concurrent well formed history has a linearization $H' \in {\mathcal H}$,
% then the object is said to be \emph{linearizable}. Furthermore, if any concurrent
% history $H$ arising from algorithm $A$ can be mapped to a sequential history $H'$ such
% that $H'$ is a sequential history arising from some atomic object $o$, then we say
% that algorithm $A$ \emph{implements} a linearizable object $o$.


% \subsection{Uniform step complexity}
% \label{ivl-ssec:uni-step-complex}

% We consider the strongest progress guarantee, \emph{bounded wait-freedom}.
% An operation is bounded wait-free if whenever a process
% invokes an operation, it completes (i.e., returns a response) in a bounded number of
% that process's steps, regardless of steps taken by other processes.
% An operation's \emph{step-complexity} is the maximum number of steps
% a process takes during a single execution of this operation. We
% can convert every bounded wait-free algorithm to a \emph{uniform step complexity}
% one, in which each operation takes the exact same
% number of steps in every execution. This can be achieved by padding shorter
% execution paths with empty steps before returning. For the remainder of this paper, we consider
% algorithms with uniform step complexity.

\subsection{Randomized algorithms}
\label{ivl-ssec:rand-objects}

% We next consider quantitative \emph{randomized} algorithms, where the return value of an
% operation is drawn from some distribution. Instead of a sequential
% specification, sequential randomized objects have error analyses, for example, the distance
% between the algorithms estimate of the number of distinct elements
% to the true value~\cite{KMV},
% or the distance between the lowest priority element and the true
% lowest priority~\cite{alistarh2015spraylist}.

In randomized algorithms, processes have access to coin flips from some domain $\Omega$.
Every execution is associated with a coin flip vector $\vv{c}=(c_1, c_2, \dots)$,
where $c_i \in \Omega$ is the $i^\text{th}$ coin flip in the execution.
A \emph{randomized algorithm} $A$ is a probability distribution over deterministic
algorithms $\{A({\vv{c})}\}_{{\vv{c}} \in \Omega^{\infty}}$\footnote{We do not consider non-deterministic objects in this paper.},
arising when $A$ is instantiated with different coin flip vectors.
% Similarly, a randomized sequential specification is a distribution over deterministic sequential specifications
% $\mathcal{H}(\vv{c})$ with coin flip vectors $\vv{c} \in \Omega^\infty$.
We denote by $H(A, {\vv{c}}, \sigma)$ the history of the execution of randomized algorithm
$A$ observing coin flip vector $\vv{c}$ in schedule $\sigma$.

% In randomized algorithms, one defines a notion of an \emph{adversary}, which
% selects the schedule (recall that the schedule includes invocations with their
% parameters, hence represents the algorithm's input).
% In this paper we focus on data sketches. These are randomized algorithms that almost
% invariably assume a \emph{weak adversary}, which selects the input
% independently of the random coin flips. We focus on weak adversaries in this paper.

% Under such an adversary,
% a randomized sequential algorithm A satisfies $\mathcal{H}$ if for
% every schedule $\sigma$, under a randomly sampled coin flip
% vector $\vv{c} \in \Omega^\infty$, $H(A,\vv{c},\sigma) \in \mathcal{H}(\vv{c})$. This
% means that for a given schedule $\sigma$, the distribution of return values
% in $A(\vv{c})$ is the same as in $\mathcal{H}(\vv{c})$. 


Golab et al. show that randomized algorithms that use concurrent objects require a stronger
correctness criterion than linearizability, and propose \emph{strong linearizability}~\cite{Wojciech}.
Roughly speaking, strong linearizability stipulates that the
mapping of histories to linearizations must be prefix-preserving,
so that future coin flips cannot impact the
linearization order of earlier events. In Definition~\ref{ivl-def:sivl}, we capture this notion
by requiring that the adversary decide on linearization points before observing the coin-flips.
% In this paper we focus on data sketches. These are randomized algorithms that almost
% invariably select the input independently of the random coin flips.
% Because weak adversaries cannot adapt to past coin flips,
% we can think of all coin flips as occurring ``in the future''.
% In this model, we can formulate strong linearizability
% to require that the linearization be independent of \emph{all} coin flips.
% We formalize this condition in Section~\ref{ivl-ssec:sivl}.


% \begin{definition}[Strong linearizability]
%     Let $H^?$ be a skeleton history of some randomized algorithm $A$ with schedule $\sigma$
%     (recall that the skeleton history is the same under all coin flip vectors).
%     $H^?$ is \emph{strongly linearizable} if there exists a linearization $H'$ such that for every coin flip
%     vector $\vv{c}$: $H'(\vv{c}) \in \mathcal{H}(\vv{c})$, and for every read $R$ that returns
%     in $H^?$: $\text{ret}(R, {H^?}(\vv{c})) = \text{ret}(R, {H'}(\vv{c}))$.

%     Algorithm $A$ is an \emph{strongly linearizable implementation} of a sequential specification distribution $\{\mathcal{H}\}_{\vv{c} \in \Omega^\infty}$ if every
%     history of a well-formed execution of $A$ is strongly linearizable with respect to $\{\mathcal{H}\}_{\vv{c} \in \Omega^\infty}$.
% \end{definition}

% An algorithm $A$ is a strongly linearizable implementation of object $o$ if
% for every history $H$ there exists a linearization $H' \in \mathcal{H}(\vv{c})_o$, and for every prefix
% $G$ of $H$ there exists a linearization $G' \in \mathcal{H}(\vv{c})_o$ of $G$ 
% such that $G'$ is a prefix of $H'$.

% In quantitative objects, randomization generally induces an \emph{error} on
% returned values. Further difficulty arises
% in analyzing the effect of parallelization on the error. Golab et al.
% show that linearization does not suffice for randomized objects~\cite{Wojciech}.
% They show that given the sequential error of a randomized object, the error guarantees
% do not hold for a linearizable one. Instead, they define
% define \emph{strong linearizability}~\cite{Wojciech}, a correctness
% criterion designed for randomized algorithms.
%Strong linearizability implies that
%the error analysis of the sequential object carries over to the concurrent one,
%as a strong adversary has the same strength for both a strongly linearizable implementation
%of an object and the same atomic one.
